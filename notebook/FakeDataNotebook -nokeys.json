{
	"name": "FakeDataNotebook -nokeys",
	"properties": {
		"folder": {
			"name": "Miscellaneous"
		},
		"nbformat": 0,
		"nbformat_minor": 0,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "c7dcc7dc-7c07-426e-8978-41c02c4da00a"
					}
				},
				"source": [
					"#Read the file from Blob Storage\n",
					"storage_account_name = \"EnterYourAccountName\"\n",
					"storage_account_access_key = \"EnterYourAccessKeys\"\n",
					"blob_container = \"BlobContaioner\"\n",
					"file_name = \"sample_dataset.csv\"\n",
					"\n",
					"spark.conf.set(\n",
					"  \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n",
					"  storage_account_access_key)\n",
					"\n",
					"blob_location = \"wasbs://\"+blob_container +\"@\" + storage_account_name + \".blob.core.windows.net/\"+file_name\n",
					"\n",
					"#Upload file into data frame\n",
					"df = spark.read.format(\"csv\").load(\"wasbs://\"+blob_container +\"@\" + storage_account_name + \".blob.core.windows.net/\",header='true', inferSchema = True)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "f83a2bd8-bb3e-4397-9061-ffa55a77e6b4"
					}
				},
				"source": [
					"df.show()"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "2dcefec0-41d5-4bd8-a658-3313d3598314"
					}
				},
				"source": [
					"#Convert to Dictionary\n",
					"import pandas as pd\n",
					"\n",
					"result_pdf = df.select(\"*\").toPandas()\n",
					"records = result_pdf.to_dict(orient='records')\n",
					"\n",
					"records"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "5dc887d8-b195-488e-8405-839151893b66"
					}
				},
				"source": [
					"%sh\n",
					"/databricks/python/bin/pip install --upgrade pip"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "42ffdd79-fb71-4f26-9e5d-2da05db7293d"
					}
				},
				"source": [
					"%sh pip install Faker unicodecsv"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "15d63f37-2da3-4733-8fac-5fa09056af33"
					}
				},
				"source": [
					"#!/usr/bin/python"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "d7993a57-9eeb-46c0-918f-3a19bfd3de46"
					}
				},
				"source": [
					"import unicodecsv as csv\n",
					"from faker import Factory\n",
					"import collections\n",
					"import numpy as np\n",
					"\n",
					"spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "4b3064ac-bc18-451e-8ae9-58032a41e37b"
					}
				},
				"source": [
					"def anonymize_rows(rows):\n",
					"    \"\"\"\n",
					"    Rows is an iterable of dictionaries that contain name and\n",
					"    email fields that need to be anonymized.\n",
					"    \"\"\"\n",
					"    # Load faker\n",
					"    faker  = Factory.create()       \n",
					"    # Create mappings of names, emails, social security numbers, and phone numbers to faked names & emails.\n",
					"    names  = collections.defaultdict(faker.name)\n",
					"    firstnames = collections.defaultdict(faker.first_name)\n",
					"    lastnames = collections.defaultdict(faker.last_name)\n",
					"    emails = collections.defaultdict(faker.email)    \n",
					"    phone_numbers = collections.defaultdict(faker.phone_number)\n",
					"    phone_numbers2 = collections.defaultdict(faker.phone_number)\n",
					"\n",
					"    # Iterate over the rows from the file and yield anonymized rows.\n",
					"    for row in rows:\n",
					"        # Replace name and email fields with faked fields.\n",
					"        row[\"first_name\"] = firstnames[row[\"first_name\"]]\n",
					"        row[\"last_name\"] = lastnames[row[\"last_name\"]]\n",
					"        row[\"email\"] = emails[row[\"email\"]]        \n",
					"        row[\"phone1\"] = phone_numbers[row[\"phone1\"]]\n",
					"        row[\"phone2\"] = phone_numbers2[row[\"phone2\"]]\n",
					"\n",
					"\n",
					"        # Yield the row back to the caller\n",
					"        yield row"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "b6ec2150-8a78-4133-bd92-eafd6c0ebf3f"
					}
				},
				"source": [
					"fieldnames = ['first_name', 'last_name','company_name','address','city','county','state','zip','phone1','phone2','email','web']                  \n",
					"#cleaned = pd.DataFrame(columns=['first_name', 'last_name','company_name','address','city','county','state','zip','phone1','phone2','email','web'])\n",
					"\n",
					"rows_list = []\n",
					"# Read and anonymize data, writing to target file.\n",
					"for row in anonymize_rows(records):  \n",
					"  dict1 = {}\n",
					"  # get input row in dictionary format\n",
					"  # key = col_name\n",
					"  dict1.update(row) \n",
					"\n",
					"  rows_list.append(dict1)\n",
					"\n",
					" \n",
					"cleaned = pd.DataFrame(rows_list)    \n",
					"cleaned"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "039e0a86-3fdc-48d9-8582-0f4a006720c7"
					}
				},
				"source": [
					"# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
					"cleanedDf = spark.createDataFrame(cleaned)\n",
					"cleanedDf.show()"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "8421ec6f-e4e2-4833-8900-1fd569fb6cb5"
					}
				},
				"source": [
					"cleanedDf.write.format(\"jdbc\").option(\"url\", \"jdbc:sqlserver://YOURSQLSERVER.database.windows.net:1433\").option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\").option(\"database\",\"YOURDATABASENAME\").option(\"dbtable\", \"NONEXISTINGTEMPORARYTABLE\").option(\"user\", \"USERNAME\").option(\"password\", \"YOURPASSWORD\").save()"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "9763b23d-c890-4609-85f4-3a2fd4e24fee"
					}
				},
				"source": [
					""
				],
				"attachments": null,
				"execution_count": 0
			}
		]
	}
}