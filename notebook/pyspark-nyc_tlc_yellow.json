{
	"name": "pyspark-nyc_tlc_yellow",
	"properties": {
		"folder": {
			"name": "NYC-TLC"
		},
		"nbformat": 4,
		"nbformat_minor": 0,
		"bigDataPool": {
			"referenceName": "NotebookPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "112g",
			"driverCores": 16,
			"executorMemory": "112g",
			"executorCores": 16,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/20e228e4-63c7-480a-a860-4b61e9623727/resourceGroups/EDU-US-DevPoc/providers/Microsoft.Synapse/workspaces/educsademo/bigDataPools/NotebookPool",
				"name": "NotebookPool",
				"type": "Spark",
				"endpoint": "https://educsademo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/NotebookPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 16,
				"memory": 112,
				"extraHeader": null
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"scrolled": false
				},
				"source": [
					"# Azure storage access info\n",
					"blob_account_name = \"edupocdatalake\"\n",
					"blob_container_name = \"nyctlc\"\n",
					"blob_relative_path = \"yellow\"\n",
					"blob_sas_token = r\"\""
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"# Allow SPARK to read from Blob remotely\n",
					"#wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
					"wasbs_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
					"#wasbs_path = 'abfss://nyctlc@edupocdatalake.dfs.core.windows.net/yellow'\n",
					"spark.conf.set(\n",
					"  'fs.azure.sas.%s.%s.dfs.core.windows.net' % (blob_container_name, blob_account_name),\n",
					"  blob_sas_token)\n",
					"#spark.conf.set(\n",
					"#  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
					"#  blob_sas_token)\n",
					"print('Remote blob path: ' + wasbs_path)"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"scrolled": true
				},
				"source": [
					"# SPARK read parquet, note that it won't load any data yet by now\n",
					"df = spark.read.parquet(wasbs_path)\n",
					"print('Register the DataFrame as a SQL temporary view: source')\n",
					"df.createOrReplaceTempView('source')"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"vendorID"
							],
							"values": [
								"tpepPickupDateTime"
							],
							"yLabel": "tpepPickupDateTime",
							"xLabel": "vendorID",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"tpepPickupDateTime\":{\"1\":0,\"2\":0}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Display top 10 rows\n",
					"print('Displaying top 10 rows: ')\n",
					"display(spark.sql('SELECT * FROM source LIMIT 10'))"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\n",
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS nyctaxi\")\n",
					"val df = spark.sql(\"SELECT * FROM source\") \n",
					"df.write.mode(\"overwrite\").saveAsTable(\"nyctaxi.trip1\")"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"SumTripDistance"
							],
							"values": [
								"PassengerCount"
							],
							"yLabel": "PassengerCount",
							"xLabel": "SumTripDistance",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"PassengerCount\":{\"1062.5700000000002\":6,\"1427.2800000000002\":3,\"21879.320000000003\":1,\"2354.4600000000005\":5,\"4747.21\":2,\"929.83\":4}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"df = spark.sql(\"\"\"\n",
					"   SELECT PassengerCount,\n",
					"       SUM(TripDistance) as SumTripDistance,\n",
					"       AVG(TripDistance) as AvgTripDistance\n",
					"   FROM nyctaxi.trip1\n",
					"   WHERE TripDistance > 0 AND PassengerCount > 0\n",
					"   GROUP BY PassengerCount\n",
					"   ORDER BY PassengerCount\n",
					"\"\"\") \n",
					"display(df)\n",
					"df.write.saveAsTable(\"nyctaxi.passengercountstats1\")"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\n",
					"val df = spark.sql(\"SELECT * FROM nyctaxi.passengercountstats1\")\n",
					"df.write.sqlanalytics(\"SQLPOOL.dbo.PassengerCountStats1\", Constants.INTERNAL )"
				],
				"attachments": null,
				"execution_count": 8
			}
		]
	}
}